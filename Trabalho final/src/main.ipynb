{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hdbscan\n",
    "import debacl\n",
    "import fastcluster\n",
    "import sklearn.cluster\n",
    "import scipy\n",
    "import scipy.cluster\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.metrics.cluster\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import munkres\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_palette('Paired', 10)\n",
    "sns.set_color_codes()\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "   os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename, className):\n",
    "  script_dir = os.getcwd()\n",
    "  data_dir = os.path.join(script_dir, '..', 'data')\n",
    "  file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "  try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    dataOnly = data.drop(className, axis='columns')\n",
    "    labelsOnly = data[className]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    dict = {'Filename': filename,\n",
    "        'DataOnly': dataOnly,\n",
    "        'Data': data,\n",
    "        'LabelsOnly': encoder.fit_transform(labelsOnly),\n",
    "        'LabelsEncoded': encoder.classes_,\n",
    "        'LabelsOnlyEncoded': labelsOnly\n",
    "        }\n",
    "\n",
    "    return dict\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Arquivo {filename} não encontrado no diretório {data_dir}\")\n",
    "    return None\n",
    "  except pd.errors.EmptyDataError:\n",
    "    print(f\"Arquivo {filename} está vazio\")\n",
    "    return None\n",
    "  except pd.errors.ParserError:\n",
    "    print(f\"Erro ao analisar o arquivo {filename}\")\n",
    "    return None\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          printNumberFormatGraph = '0.4f',\n",
    "                          printNumberFormatLegend = None):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, (\"{:\"+printNumberFormatGraph+\"}\").format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    if(printNumberFormatLegend == None):\n",
    "      printNumberFormatLegend = printNumberFormatGraph\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel(('Predicted label\\naccuracy={:'+printNumberFormatLegend+'}; misclass={:'+printNumberFormatLegend+'}').format(accuracy, misclass))\n",
    "    #plt.show()\n",
    "    return plt;\n",
    "\n",
    "def translateLabels(masterList, listToConvert):    \n",
    "  contMatrix = sklearn.metrics.cluster.contingency_matrix(masterList, listToConvert)\n",
    "  labelMatcher = munkres.Munkres()\n",
    "  labelTranlater = labelMatcher.compute(contMatrix.max() - contMatrix)\n",
    "\n",
    "  uniqueLabels1 = list(set(masterList))\n",
    "  uniqueLabels2 = list(set(listToConvert))\n",
    "\n",
    "  tranlatorDict = {}\n",
    "  for thisPair in labelTranlater:\n",
    "    tranlatorDict[uniqueLabels2[thisPair[1]]] = uniqueLabels1[thisPair[0]]\n",
    "\n",
    "  return {\"TranslateList\" : [tranlatorDict[label] for label in listToConvert], \"ContingencyMatrix\":contMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.csv\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "data= load_file(\"iris.csv\", \"class\")\n",
    "print(data[\"Filename\"])\n",
    "#print(data[\"DataOnly\"].head())\n",
    "#print(data[\"Data\"].head())\n",
    "#print(data[\"LabelsOnly\"].head())\n",
    "print(data[\"LabelsEncoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_algorithm(datasetName, dataset, cluster_function_name, cluster_function, function_args, function_kwds, sample_size=2):\n",
    "    start_time = time.time()\n",
    "    cluster = cluster_function(dataset[\"DataOnly\"], *function_args, **function_kwds)\n",
    "    time_taken = time.time() - start_time\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit_transform(cluster.labels_),\n",
    "    classes = encoder.classes_\n",
    "\n",
    "    #Algoritmo de Hungarian:\n",
    "    #Encontra a melhor combinação de Clusters entre 2 agrupamentos diferentes\n",
    "\n",
    "    confusion_matrix = sklearn.metrics._classification.confusion_matrix(dataset[\"LabelsOnly\"], cluster.labels_, normalize='all')\n",
    "    # Calcular a matriz de custo como o negativo da matriz de confusão\n",
    "    cost_matrix = -confusion_matrix\n",
    "\n",
    "    # Aplicar o algoritmo de Hungarian para encontrar o mapeamento ótimo\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    # Mapear os rótulos antigos para os novos usando o mapeamento encontrado\n",
    "    clusterLabels = cluster.labels_.astype(\"object\")\n",
    "    for original, new in enumerate(col_ind):\n",
    "        clusterLabels[cluster.labels_ == original] = dataset['LabelsEncoded'][new]\n",
    "\n",
    "    #transLabels = translateLabels(dataset[\"LabelsOnly\"], cluster.labels_)\n",
    "    #contingency = transLabels[\"ContingencyMatrix\"]\n",
    "    #clusterLabels = transLabels[\"TranslateList\"]\n",
    "    \n",
    "    #clusterLabels = np.asarray(clusterLabels, dtype='object')\n",
    "\n",
    "    #for idx, x in enumerate(dataset[\"LabelsEncoded\"]):\n",
    "    #    clusterLabels[clusterLabels==idx] = x;\n",
    "\n",
    "    confusion_matrix = sklearn.metrics._classification.confusion_matrix(dataset[\"LabelsOnlyEncoded\"], clusterLabels, normalize='all')\n",
    "    cmd = plot_confusion_matrix(cm=confusion_matrix,\n",
    "                                target_names=dataset[\"LabelsEncoded\"],\n",
    "                                title='Confusion matrix - '+datasetName+' - '+cluster_function_name,\n",
    "                                printNumberFormatGraph=\".0%\",\n",
    "                                printNumberFormatLegend=\".1%\")\n",
    "    \n",
    "    purity_score = np.sum(np.amax(confusion_matrix, axis=0)) / np.sum(confusion_matrix)\n",
    "    jaccard_score = sklearn.metrics._classification.jaccard_score(dataset[\"LabelsOnlyEncoded\"], clusterLabels, average='micro')\n",
    "    adjusted_rand_score = sklearn.metrics.cluster.adjusted_rand_score(dataset[\"LabelsOnlyEncoded\"], clusterLabels)\n",
    "    accuracy_score = sklearn.metrics._classification.accuracy_score(dataset[\"LabelsOnlyEncoded\"], clusterLabels)\n",
    "\n",
    "    classification_report = sklearn.metrics.classification_report(dataset[\"LabelsOnlyEncoded\"], clusterLabels, target_names=dataset[\"LabelsEncoded\"], zero_division=0)\n",
    "\n",
    "    dict = {'ClusterFunctionName': cluster_function_name,\n",
    "        'Cluster': cluster,\n",
    "        'TimeTaken': time_taken,\n",
    "        'ClusterLabels': clusterLabels,\n",
    "        'AccuracyScore': accuracy_score,\n",
    "        'PurityScore': purity_score,\n",
    "        'AdjustedRandScore': adjusted_rand_score,\n",
    "        'JaccardScore': jaccard_score,\n",
    "        #'ContingencyMatrix': contingency,\n",
    "        'ConfusionMatrix': confusion_matrix,\n",
    "        'ClassificationReport': classification_report,\n",
    "        'ConfusionMatrixDisplay': cmd\n",
    "        }\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mRice Cammeo Osmancik Dataset\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#'''\u001b[39;00m\n\u001b[0;32m     19\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mDBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m clusterResult\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbenchmark_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDBScan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#'''\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03magglomerative = sklearn.cluster.AgglomerativeClustering(len(data[\"LabelsEncoded\"]))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mclusterResult.append(benchmark_algorithm(dataKey, data, \"Agglomerative Clustering\", agglomerative.fit, (), {}))\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m#'''\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mbenchmark_algorithm\u001b[1;34m(datasetName, dataset, cluster_function_name, cluster_function, function_args, function_kwds, sample_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m clusterLabels \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m original, new \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(col_ind):\n\u001b[1;32m---> 23\u001b[0m     clusterLabels[cluster\u001b[38;5;241m.\u001b[39mlabels_ \u001b[38;5;241m==\u001b[39m original] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLabelsEncoded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#transLabels = translateLabels(dataset[\"LabelsOnly\"], cluster.labels_)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#contingency = transLabels[\"ContingencyMatrix\"]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#clusterLabels = transLabels[\"TranslateList\"]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#for idx, x in enumerate(dataset[\"LabelsEncoded\"]):\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#    clusterLabels[clusterLabels==idx] = x;\u001b[39;00m\n\u001b[0;32m     34\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39m_classification\u001b[38;5;241m.\u001b[39mconfusion_matrix(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabelsOnlyEncoded\u001b[39m\u001b[38;5;124m\"\u001b[39m], clusterLabels, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "dataList = {\n",
    "        #'Iris': \"iris.csv\",\n",
    "        'Rice Cammeo Osmancik': \"Rice_Cammeo_Osmancik.arff.csv\"\n",
    "        }\n",
    "\n",
    "for dataKey in dataList:\n",
    "    print(color.BOLD + color.UNDERLINE + dataKey + \" Dataset\" + color.END + \"\\n\")\n",
    "    \n",
    "    data = load_file(dataList[dataKey], \"class\")\n",
    "\n",
    "    clusterResult = [];\n",
    "\n",
    "    '''\n",
    "    k_means = sklearn.cluster.KMeans(len(data[\"LabelsEncoded\"]), n_init='auto', max_iter=300)\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"K-means\", k_means.fit, (), {}))\n",
    "    #'''\n",
    "\n",
    "    #'''\n",
    "    dbscan = sklearn.cluster.DBSCAN(eps=0.1)\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"DBScan\", dbscan.fit, (), {}))\n",
    "    #'''\n",
    "    '''\n",
    "    agglomerative = sklearn.cluster.AgglomerativeClustering(len(data[\"LabelsEncoded\"]))\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"Agglomerative Clustering\", agglomerative.fit, (), {}))\n",
    "    #'''\n",
    "    '''\n",
    "    spectral = sklearn.cluster.SpectralClustering(len(data[\"LabelsEncoded\"]))\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"Spectral Clustering\", spectral.fit, (), {}))\n",
    "    #'''\n",
    "    '''\n",
    "    birch = sklearn.cluster.Birch(n_clusters=len(data[\"LabelsEncoded\"]), threshold=0.3)\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"Birch\", birch.fit, (), {}))\n",
    "    #'''\n",
    "    '''\n",
    "    meanShift = sklearn.cluster.MeanShift()\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"Mean Shift\", meanShift.fit, (), {}))\n",
    "    #'''\n",
    "\n",
    "    #clusterResult.append(benchmark_algorithm(dataKey, data, \"Fast Cluster\", fastcluster.linkage_vector, (), {}))\n",
    "    \n",
    "    #clusterResult.append(benchmark_algorithm(dataKey, data, \"Hierarchy\", scipy.cluster.hierarchy.single, (), {}))    \n",
    "\n",
    "    '''Não usar\n",
    "    optics= sklearn.cluster.OPTICS()\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"OPTICS\", optics.fit, (), {}))\n",
    "    #'''\n",
    "\n",
    "    '''Não usar\n",
    "    affinity_prop = sklearn.cluster.AffinityPropagation(random_state=5)\n",
    "    clusterResult.append(benchmark_algorithm(dataKey, data, \"Affinity Propagation\", affinity_prop.fit, (), {}))\n",
    "    #'''\n",
    "\n",
    "    for dataResult in clusterResult:\n",
    "        print(color.UNDERLINE + \"Cluster Algorithm: \" + dataResult['ClusterFunctionName'] + color.END)\n",
    "        print(\"Time Taken (seg):\", dataResult['TimeTaken'])\n",
    "        print(\"Accuracy Score:\", dataResult['AccuracyScore'])\n",
    "        print(\"Purity Score:\", dataResult['PurityScore'])\n",
    "        print(\"Adjusted Rand Index:\", dataResult['AdjustedRandScore'])\n",
    "        print(\"Jaccard Score:\", dataResult['JaccardScore'])\n",
    "\n",
    "        print(dataResult['ClassificationReport'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m k_means \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mKMeans(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m k_means_data \u001b[38;5;241m=\u001b[39m benchmark_algorithm(\u001b[43mdataset_sizes\u001b[49m, k_means\u001b[38;5;241m.\u001b[39mfit, (), {})\n\u001b[0;32m      4\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mDBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.25\u001b[39m)\n\u001b[0;32m      5\u001b[0m dbscan_data \u001b[38;5;241m=\u001b[39m benchmark_algorithm(dataset_sizes, dbscan\u001b[38;5;241m.\u001b[39mfit, (), {})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "k_means = sklearn.cluster.KMeans(10)\n",
    "k_means_data = benchmark_algorithm(dataset_sizes, k_means.fit, (), {})\n",
    "\n",
    "dbscan = sklearn.cluster.DBSCAN(eps=1.25)\n",
    "dbscan_data = benchmark_algorithm(dataset_sizes, dbscan.fit, (), {})\n",
    "\n",
    "scipy_k_means_data = benchmark_algorithm(dataset_sizes,\n",
    "                                         scipy.cluster.vq.kmeans, (10,), {})\n",
    "\n",
    "scipy_single_data = benchmark_algorithm(dataset_sizes,\n",
    "                                        scipy.cluster.hierarchy.single, (), {})\n",
    "\n",
    "fastclust_data = benchmark_algorithm(dataset_sizes,\n",
    "                                     fastcluster.linkage_vector, (), {})\n",
    "\n",
    "hdbscan_ = hdbscan.HDBSCAN()\n",
    "hdbscan_data = benchmark_algorithm(dataset_sizes, hdbscan_.fit, (), {})\n",
    "\n",
    "debacl_data = benchmark_algorithm(dataset_sizes,\n",
    "                                  debacl.geom_tree.geomTree, (5, 5), {'verbose':False})\n",
    "\n",
    "agglomerative = sklearn.cluster.AgglomerativeClustering(10)\n",
    "agg_data = benchmark_algorithm(dataset_sizes,\n",
    "                               agglomerative.fit, (), {}, sample_size=4)\n",
    "\n",
    "spectral = sklearn.cluster.SpectralClustering(10)\n",
    "spectral_data = benchmark_algorithm(dataset_sizes,\n",
    "                                    spectral.fit, (), {}, sample_size=6)\n",
    "\n",
    "affinity_prop = sklearn.cluster.AffinityPropagation()\n",
    "ap_data = benchmark_algorithm(dataset_sizes,\n",
    "                              affinity_prop.fit, (), {}, sample_size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
