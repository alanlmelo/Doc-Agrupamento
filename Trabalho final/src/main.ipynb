{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hdbscan\n",
    "import debacl\n",
    "import fastcluster\n",
    "import sklearn.cluster\n",
    "import scipy.cluster\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.metrics.cluster\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_palette('Paired', 10)\n",
    "sns.set_color_codes()\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename, className):\n",
    "  script_dir = os.getcwd()\n",
    "  data_dir = os.path.join(script_dir, '..', 'data')\n",
    "  file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "  try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    dataOnly = data.drop(className, axis='columns')\n",
    "    labelsOnly = data[className]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    dict = {'Filename': filename,\n",
    "        'DataOnly': dataOnly,\n",
    "        'Data': data,\n",
    "        'LabelsOnly': encoder.fit_transform(labelsOnly),\n",
    "        'LabelsEncoded': encoder.classes_,\n",
    "        'LabelsOnlyEncoded': labelsOnly\n",
    "        }\n",
    "\n",
    "    return dict\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Arquivo {filename} não encontrado no diretório {data_dir}\")\n",
    "    return None\n",
    "  except pd.errors.EmptyDataError:\n",
    "    print(f\"Arquivo {filename} está vazio\")\n",
    "    return None\n",
    "  except pd.errors.ParserError:\n",
    "    print(f\"Erro ao analisar o arquivo {filename}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.csv\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "data= load_file(\"iris.csv\", \"class\")\n",
    "print(data[\"Filename\"])\n",
    "#print(data[\"DataOnly\"].head())\n",
    "#print(data[\"Data\"].head())\n",
    "#print(data[\"LabelsOnly\"].head())\n",
    "print(data[\"LabelsEncoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_algorithm(dataset, cluster_function_name, cluster_function, function_args, function_kwds, sample_size=2):\n",
    "    start_time = time.time()\n",
    "    cluster = cluster_function(dataset[\"DataOnly\"], *function_args, **function_kwds)\n",
    "    time_taken = time.time() - start_time\n",
    "    contingency = sklearn.metrics.cluster.contingency_matrix(dataset[\"LabelsOnly\"], cluster.labels_)\n",
    "    confusion_matrix = sklearn.metrics._classification.confusion_matrix(dataset[\"LabelsOnly\"], cluster.labels_)\n",
    "    var = []\n",
    "    for n in contingency:\n",
    "        var.append(np.where(n == max(n))[0][0])\n",
    "    clusterLabels = cluster.labels_.copy().astype('object')\n",
    "    for n in var:\n",
    "        clusterLabels[clusterLabels==n]=dataset[\"LabelsEncoded\"][n]\n",
    "\n",
    "    purity_score = np.sum(np.amax(confusion_matrix, axis=0)) / np.sum(confusion_matrix)\n",
    "    jaccard_score = sklearn.metrics._classification.jaccard_score(dataset[\"LabelsOnly\"], cluster.labels_, average='micro')\n",
    "    adjusted_rand_score = sklearn.metrics.cluster.adjusted_rand_score(dataset[\"LabelsOnly\"], cluster.labels_)\n",
    "    accuracy_score = sklearn.metrics._classification.accuracy_score(dataset[\"LabelsOnly\"], cluster.labels_)\n",
    "\n",
    "    classification_report = sklearn.metrics.classification_report(dataset[\"LabelsOnly\"], cluster.labels_, target_names=dataset[\"LabelsEncoded\"], zero_division=0)\n",
    "\n",
    "    dict = {'ClusterFunctionName': cluster_function_name,\n",
    "        'Cluster': cluster,\n",
    "        'TimeTaken': time_taken,\n",
    "        'ClusterLabels': clusterLabels,\n",
    "        'AccuracyScore': accuracy_score,\n",
    "        'PurityScore': purity_score,\n",
    "        'AdjustedRandScore': adjusted_rand_score,\n",
    "        'JaccardScore': jaccard_score,\n",
    "        'ClassificationReport': classification_report\n",
    "        }\n",
    "\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRice Cammeo Osmancik Dataset\u001b[0m\n",
      "\n",
      "\u001b[4mCluster Algorithm: K-means\u001b[0m\n",
      "Time Taken (seg): 0.008510828018188477\n",
      "Accuracy Score: 0.9149606299212598\n",
      "Purity Score: 0.9149606299212598\n",
      "Adjusted Rand Index: 0.6885026205971801\n",
      "Jaccard Score: 0.8432510885341074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cammeo       0.91      0.89      0.90      1630\n",
      "    Osmancik       0.92      0.94      0.93      2180\n",
      "\n",
      "    accuracy                           0.91      3810\n",
      "   macro avg       0.91      0.91      0.91      3810\n",
      "weighted avg       0.91      0.91      0.91      3810\n",
      "\n",
      "\u001b[4mCluster Algorithm: DBScan\u001b[0m\n",
      "Time Taken (seg): 0.31294941902160645\n",
      "Accuracy Score: 0.42782152230971127\n",
      "Purity Score: 0.5721784776902887\n",
      "Adjusted Rand Index: 0.0\n",
      "Jaccard Score: 0.27212020033388984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cammeo       0.43      1.00      0.60      1630\n",
      "    Osmancik       0.00      0.00      0.00      2180\n",
      "\n",
      "    accuracy                           0.43      3810\n",
      "   macro avg       0.21      0.50      0.30      3810\n",
      "weighted avg       0.18      0.43      0.26      3810\n",
      "\n",
      "\u001b[4mCluster Algorithm: Agglomerative Clustering\u001b[0m\n",
      "Time Taken (seg): 0.2781338691711426\n",
      "Accuracy Score: 0.9173228346456693\n",
      "Purity Score: 0.9173228346456693\n",
      "Adjusted Rand Index: 0.696493232388751\n",
      "Jaccard Score: 0.8472727272727273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cammeo       0.88      0.93      0.91      1630\n",
      "    Osmancik       0.95      0.91      0.93      2180\n",
      "\n",
      "    accuracy                           0.92      3810\n",
      "   macro avg       0.91      0.92      0.92      3810\n",
      "weighted avg       0.92      0.92      0.92      3810\n",
      "\n",
      "\u001b[4mCluster Algorithm: Spectral Clustering\u001b[0m\n",
      "Time Taken (seg): 1.7339634895324707\n",
      "Accuracy Score: 0.08845144356955381\n",
      "Purity Score: 0.9115485564304462\n",
      "Adjusted Rand Index: 0.67715520103104\n",
      "Jaccard Score: 0.04627214060140052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cammeo       0.09      0.13      0.11      1630\n",
      "    Osmancik       0.08      0.06      0.07      2180\n",
      "\n",
      "    accuracy                           0.09      3810\n",
      "   macro avg       0.09      0.09      0.09      3810\n",
      "weighted avg       0.09      0.09      0.08      3810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataList = {\n",
    "        #'Iris': \"iris.csv\",\n",
    "        'Rice Cammeo Osmancik': \"Rice_Cammeo_Osmancik.arff.csv\"\n",
    "        }\n",
    "\n",
    "for dataKey in dataList:\n",
    "    print(color.BOLD + dataKey + \" Dataset\" + color.END + \"\\n\")\n",
    "    \n",
    "    data = load_file(dataList[dataKey], \"class\")\n",
    "\n",
    "    clusterResult = [];\n",
    "\n",
    "    k_means = sklearn.cluster.KMeans(len(data[\"LabelsEncoded\"]), n_init='auto')\n",
    "    clusterResult.append(benchmark_algorithm(data, \"K-means\", k_means.fit, (), {}))\n",
    "\n",
    "    dbscan = sklearn.cluster.DBSCAN(eps=1.25)\n",
    "    clusterResult.append(benchmark_algorithm(data, \"DBScan\", dbscan.fit, (), {}))\n",
    "\n",
    "    agglomerative = sklearn.cluster.AgglomerativeClustering(len(data[\"LabelsEncoded\"]))\n",
    "    clusterResult.append(benchmark_algorithm(data, \"Agglomerative Clustering\", agglomerative.fit, (), {}))\n",
    "\n",
    "    spectral = sklearn.cluster.SpectralClustering(len(data[\"LabelsEncoded\"]))\n",
    "    clusterResult.append(benchmark_algorithm(data, \"Spectral Clustering\", spectral.fit, (), {}))\n",
    "\n",
    "    #affinity_prop = sklearn.cluster.AffinityPropagation()\n",
    "    #clusterResult.append(benchmark_algorithm(data, \"Affinity Propagation\", affinity_prop.fit, (), {}))\n",
    "\n",
    "    for dataResult in clusterResult:\n",
    "        print(color.UNDERLINE + \"Cluster Algorithm: \" + dataResult['ClusterFunctionName'] + color.END)\n",
    "        print(\"Time Taken (seg):\", dataResult['TimeTaken'])\n",
    "        print(\"Accuracy Score:\", dataResult['AccuracyScore'])\n",
    "        print(\"Purity Score:\", dataResult['PurityScore'])\n",
    "        print(\"Adjusted Rand Index:\", dataResult['AdjustedRandScore'])\n",
    "        print(\"Jaccard Score:\", dataResult['JaccardScore'])\n",
    "        print(dataResult['ClassificationReport'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m k_means \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mKMeans(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m k_means_data \u001b[38;5;241m=\u001b[39m benchmark_algorithm(\u001b[43mdataset_sizes\u001b[49m, k_means\u001b[38;5;241m.\u001b[39mfit, (), {})\n\u001b[0;32m      4\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mDBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.25\u001b[39m)\n\u001b[0;32m      5\u001b[0m dbscan_data \u001b[38;5;241m=\u001b[39m benchmark_algorithm(dataset_sizes, dbscan\u001b[38;5;241m.\u001b[39mfit, (), {})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "k_means = sklearn.cluster.KMeans(10)\n",
    "k_means_data = benchmark_algorithm(dataset_sizes, k_means.fit, (), {})\n",
    "\n",
    "dbscan = sklearn.cluster.DBSCAN(eps=1.25)\n",
    "dbscan_data = benchmark_algorithm(dataset_sizes, dbscan.fit, (), {})\n",
    "\n",
    "scipy_k_means_data = benchmark_algorithm(dataset_sizes,\n",
    "                                         scipy.cluster.vq.kmeans, (10,), {})\n",
    "\n",
    "scipy_single_data = benchmark_algorithm(dataset_sizes,\n",
    "                                        scipy.cluster.hierarchy.single, (), {})\n",
    "\n",
    "fastclust_data = benchmark_algorithm(dataset_sizes,\n",
    "                                     fastcluster.linkage_vector, (), {})\n",
    "\n",
    "hdbscan_ = hdbscan.HDBSCAN()\n",
    "hdbscan_data = benchmark_algorithm(dataset_sizes, hdbscan_.fit, (), {})\n",
    "\n",
    "debacl_data = benchmark_algorithm(dataset_sizes,\n",
    "                                  debacl.geom_tree.geomTree, (5, 5), {'verbose':False})\n",
    "\n",
    "agglomerative = sklearn.cluster.AgglomerativeClustering(10)\n",
    "agg_data = benchmark_algorithm(dataset_sizes,\n",
    "                               agglomerative.fit, (), {}, sample_size=4)\n",
    "\n",
    "spectral = sklearn.cluster.SpectralClustering(10)\n",
    "spectral_data = benchmark_algorithm(dataset_sizes,\n",
    "                                    spectral.fit, (), {}, sample_size=6)\n",
    "\n",
    "affinity_prop = sklearn.cluster.AffinityPropagation()\n",
    "ap_data = benchmark_algorithm(dataset_sizes,\n",
    "                              affinity_prop.fit, (), {}, sample_size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
